---
permalink: /projects/
title: "Projects"
layout: single
author_profile: true
---


### **Construction of Large Language Model Inference System**  
*02-06/2025*  
- Built a multi-model architecture based on open-source models such as Mistral-7B, Yi-34B, and LLaMA 2, using Zero-shot, Few-shot, and SFT fine-tuning strategies, combined with H2O LLM Studio, to optimize the model, achieving an accuracy improvement of over 10% in specific fields such as physics/biology  
- Used LoRA fine-tuning and cached past_key_values, increasing inference speed by 60%  
- Dynamically enhanced context by integrating RAG and LangChain, building a corpus containing 60 million paragraphs based on multi-source Wikipedia data, developing a FAISS vector retrieval system, and entering pre-designed multi-round Prompt templates  
- Implemented confidence integration based on results of multiple models, combined with methods such as TF-IDF re-ranking and Embedding similarity, to increase the Top3 hit rate to 93%; and the integrated model was 15% more accurate than the single model  

### **Identification of Offensive Language in Chinese Social Media Environment**  
*10-12/2024*  
- Conducted the collection, cleaning, and annotation of Chinese social media data; built and optimized multiple dedicated datasets including COLD, ToxiCN, and ToxiCloakCN  
- Built and trained LSTM neural networks and pre-trained language models (BERT and RoBERTa) to achieve accurate identification and classification of aggressive language  
- Evaluated and optimized the model through performance evaluation indicators (Accuracy, ROC-AUC, F1-score); eventually decided on the RoBERTa model, which achieved an accuracy of 78.54% and performed outstandingly in complex language recognition  

### **Predictive Analysis of Diamond Prices**  
*05-06/2024*  
- Used Python and R languages to model and analyze 5000 pieces of diamond transaction data, attempting to build a high-precision price prediction model  
- Implemented data cleaning, feature engineering, and multivariate transformation (Box-Cox) to optimize model performance  
- Explored data features through data visualization tools, identified multicollinearity between variables, and improved model prediction accuracy  
- Adopted stepwise regression and cross-validation methods; the final model achieved an RÂ² of 0.9853 and a cross-validation RMSE of 0.1308, showing superior and stable performance  
- The model successfully passed regression assumption tests such as residual independence, normal distribution, and homoscedasticity to ensure the robustness of predictions  

### **Processing and Analyzing of Earthquake Data from 1900 to 2023**  
*03-05/2023*  
- Utilized big data frameworks such as Hadoop and Spark for large-scale data storage and parallel computing, completing efficient processing and analysis of global long-time series earthquake data  
- Performed data cleaning and time format standardization, used reverse geocoding technology to obtain accurate geographical information, and conducted in-depth analysis of the spatiotemporal distribution characteristics of earthquakes  
- Adopted visualization tools such as Plotly and WordCloud to clearly display the spatial hotspots and key event characteristics of earthquake data, providing strong data support for subsequent risk prediction and disaster prevention and mitigation  





